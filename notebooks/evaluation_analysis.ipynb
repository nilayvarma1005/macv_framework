{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MACV Evaluation Analysis\n",
        "\n",
        "This notebook analyzes the results from the `msv_evaluation_results.csv` file and generates the plots for the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the results\n",
        "df = pd.read_csv('../data/msv_evaluation_results.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Accuracy Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_data = {\n",
        "    'Single LLM': df.groupby('dataset')['single_llm_correct'].mean(),\n",
        "    'RAG': df.groupby('dataset')['rag_correct'].mean(),\n",
        "    'Self-Correction': df.groupby('dataset')['self_correction_correct'].mean(),\n",
        "    'MACV': df.groupby('dataset')['msv_Full_MSV_correct'].mean()\n",
        "}\n",
        "\n",
        "accuracy_df = pd.DataFrame(accuracy_data)\n",
        "\n",
        "accuracy_df.plot(kind='bar', figsize=(12, 7))\n",
        "plt.title('Accuracy Comparison Across Models and Datasets')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Dataset')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.savefig('../images/1_accuracy.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Hallucination Rate\n",
        "Hallucination is defined as `1 - accuracy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hallucination_df = 1 - accuracy_df\n",
        "\n",
        "hallucination_df.plot(kind='bar', figsize=(12, 7))\n",
        "plt.title('Hallucination Rate Comparison')\n",
        "plt.ylabel('Hallucination Rate')\n",
        "plt.xlabel('Dataset')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.savefig('../images/2_hallucination.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Average Response Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_data = {\n",
        "    'Single LLM': df['single_llm_time'].mean(),\n",
        "    'RAG': df['rag_time'].mean(),\n",
        "    'Self-Correction': df['self_correction_time'].mean(),\n",
        "    'MACV': df['msv_Full_MSV_time'].mean()\n",
        "}\n",
        "\n",
        "time_series = pd.Series(time_data)\n",
        "\n",
        "time_series.plot(kind='bar', figsize=(10, 6), color=['skyblue', 'lightgreen', 'salmon', 'plum'])\n",
        "plt.title('Average Response Time (Seconds)')\n",
        "plt.ylabel('Time (s)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.savefig('../images/3_avg_time.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. MACV Confusion Matrix\n",
        "\n",
        "This shows how well MACV's decision to `ACCEPT` or `REJECT` aligns with whether the response was actually correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true = df['msv_Full_MSV_correct'].astype(int) # 1 for correct, 0 for incorrect\n",
        "y_pred = (df['msv_Full_MSV_decision'] == 'ACCEPT').astype(int) # 1 for ACCEPT, 0 for REJECT\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Reject', 'Accept'], yticklabels=['Incorrect', 'Correct'])\n",
        "plt.title('MACV Decision Confusion Matrix')\n",
        "plt.xlabel('Predicted Decision')\n",
        "plt.ylabel('Actual Correctness')\n",
        "plt.savefig('../images/4_msv_confusion_matrix.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. MACV Rejection Reasons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rejection_df = df[df['msv_Full_MSV_decision'] == 'REJECT']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "rejection_df['msv_Full_MSV_reason'].value_counts().plot(kind='barh')\n",
        "plt.title('Reasons for MACV Rejection')\n",
        "plt.xlabel('Count')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.savefig('../images/5_msv_rejection_reasons.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
